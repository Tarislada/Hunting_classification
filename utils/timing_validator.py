"""
Timing validation for hunting behavior pipeline.
Compares pipeline predictions with expert ground truth timing annotations.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from typing import Dict, Any, List, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

class TimingValidator:
    """
    Validates pipeline timing predictions against expert ground truth.
    Compares frame-level predictions for hunting behavior transitions.
    """
    
    def __init__(self, ground_truth_csv: str):
        """
        Initialize the timing validator.
        
        Args:
            ground_truth_csv: Path to standardized ground truth CSV file
        """
        self.ground_truth_csv = ground_truth_csv
        self.ground_truth = None
        self.pipeline_results = None
        self.comparison_results = None
        
        # Behavior mapping from pipeline to ground truth events
        self.behavior_mapping = {
            'latency_to_hunt': 'chasing',      # First pursuit = first chasing detection
            'latency_to_attack': 'attack',      # Attack onset = first attack detection
            'hunting_duration': 'consumption'   # Consumption = end of hunting sequence
        }
        
        print(f"Initialized TimingValidator with ground truth: {ground_truth_csv}")
    
    def load_ground_truth(self) -> pd.DataFrame:
        """Load and validate ground truth data."""
        try:
            self.ground_truth = pd.read_csv(self.ground_truth_csv)
            print(f"Loaded ground truth: {len(self.ground_truth)} trials")
            print(f"Animals: {self.ground_truth['animal_id'].unique()}")
            print(f"Trials per animal: {self.ground_truth.groupby('animal_id')['trial_id'].nunique().to_dict()}")
            
            # Show data completeness
            timing_cols = ['latency_to_hunt', 'latency_to_attack', 'hunting_duration']
            for col in timing_cols:
                complete_count = self.ground_truth[col].notna().sum()
                total_count = len(self.ground_truth)
                print(f"  {col}: {complete_count}/{total_count} ({complete_count/total_count*100:.1f}% complete)")
            
            return self.ground_truth
            
        except Exception as e:
            raise ValueError(f"Could not load ground truth CSV: {e}")
    
    def extract_pipeline_timing(self, classification_results: pd.DataFrame, 
                               animal_id: str, trial_id: str) -> Dict[str, Optional[int]]:
        """
        Extract timing events from pipeline classification results for a single trial.
        
        Args:
            classification_results: Pipeline classification results DataFrame  
            animal_id: Animal identifier
            trial_id: Trial identifier
            
        Returns:
            dict: Extracted timing events {event_name: frame_number}
        """
        # Filter to this specific trial
        trial_data = classification_results[
            (classification_results['animal_id'] == animal_id) & 
            (classification_results['trial_id'] == trial_id)
        ].copy()
        
        if len(trial_data) == 0:
            print(f"    No pipeline data found for {animal_id} {trial_id}")
            return {
                'latency_to_hunt': None,
                'latency_to_attack': None, 
                'hunting_duration': None
            }
        
        # Sort by frame to ensure correct temporal order
        trial_data = trial_data.sort_values('frame')
        
        timing_events = {}
        
        # Extract latency to hunt (first chasing behavior)
        chasing_frames = trial_data[trial_data['behavior'] == 'chasing']
        if len(chasing_frames) > 0:
            timing_events['latency_to_hunt'] = chasing_frames['frame'].min()
        else:
            timing_events['latency_to_hunt'] = None
        
        # Extract latency to attack (first attack behavior)
        attack_frames = trial_data[trial_data['behavior'] == 'attack']
        if len(attack_frames) > 0:
            timing_events['latency_to_attack'] = attack_frames['frame'].min()
        else:
            timing_events['latency_to_attack'] = None
        
        # Extract hunting duration (last frame of hunting sequence)
        # This could be last attack frame, or we could define it differently
        # For now, let's use the last frame where behavior is not 'background'
        # hunting_frames = trial_data[trial_data['behavior'] != 'background']
        # # TODO: what if we do it as last attack frame?
        # if len(hunting_frames) > 0:
        #     timing_events['hunting_duration'] = hunting_frames['frame'].max()
        # else:
        #     timing_events['hunting_duration'] = None
        if len(attack_frames) > 0:
            timing_events['hunting_duration'] = attack_frames['frame'].max()+1
        else:
            # If there are no attack frames, the duration event is also considered missing.
            timing_events['hunting_duration'] = None
                
        return timing_events
    
    def load_pipeline_results(self, results_directory: str) -> pd.DataFrame:
        """
        Load and consolidate pipeline classification results.
        
        Args:
            results_directory: Directory containing pipeline analysis CSV files
            
        Returns:
            pd.DataFrame: Consolidated pipeline results
        """
        results_dir = Path(results_directory)
        
        # Find all analysis CSV files
        analysis_files = list(results_dir.glob('*_analysis.csv'))
        
        if not analysis_files:
            raise ValueError(f"No *_analysis.csv files found in {results_directory}")
        
        print(f"Found {len(analysis_files)} pipeline result files")
        
        all_results = []
        
        for file_path in analysis_files:
            try:
                # Extract animal and trial ID from filename
                # Assuming format like "m14_t1_validated_analysis.csv"
                filename = file_path.stem
                
                # Extract animal_id (e.g., m14, m17)
                animal_match = filename.split('_')[0]  # First part should be animal ID
                trial_match = filename.split('_')[1] if '_' in filename else 'unknown'   # Second part should be trial ID
                
                # Load the results
                df = pd.read_csv(file_path)
                
                # Add animal and trial identifiers
                df['animal_id'] = animal_match
                df['trial_id'] = trial_match
                df['source_file'] = file_path.name
                
                all_results.append(df)
                print(f"  Loaded {len(df)} frames from {animal_match} {trial_match}")
                
            except Exception as e:
                print(f"  Warning: Could not load {file_path.name}: {e}")
                continue
        
        if not all_results:
            raise ValueError("No pipeline results could be loaded")
        
        # Combine all results
        self.pipeline_results = pd.concat(all_results, ignore_index=True)
        
        print(f"Total pipeline results: {len(self.pipeline_results)} frames")
        print(f"Animals: {self.pipeline_results['animal_id'].unique()}")
        print(f"Behaviors: {self.pipeline_results['behavior'].unique()}")
        
        return self.pipeline_results
    
    def compare_timing(self) -> pd.DataFrame:
        """
        Compare pipeline timing predictions with ground truth.
        
        Returns:
            pd.DataFrame: Comparison results
        """
        if self.ground_truth is None:
            raise ValueError("Ground truth not loaded. Call load_ground_truth() first.")
        
        if self.pipeline_results is None:
            raise ValueError("Pipeline results not loaded. Call load_pipeline_results() first.")
        
        print("Comparing pipeline results with ground truth...")
        
        comparison_rows = []
        
        for _, gt_row in self.ground_truth.iterrows():
            animal_id = gt_row['animal_id']
            trial_id = gt_row['trial_id']
            
            print(f"\nProcessing {animal_id} {trial_id}:")
            
            # Extract pipeline timing for this trial
            pipeline_timing = self.extract_pipeline_timing(
                self.pipeline_results, animal_id, trial_id
            )
            
            # Compare each timing event
            comparison_row = {
                'animal_id': animal_id,
                'trial_id': trial_id,
                'notes': gt_row.get('notes', '')
            }
            
            timing_events = ['latency_to_hunt', 'latency_to_attack', 'hunting_duration']
            
            for event in timing_events:
                gt_value = gt_row[event] if pd.notna(gt_row[event]) else None
                pipeline_value = pipeline_timing[event]
                
                comparison_row[f'gt_{event}'] = gt_value
                comparison_row[f'pipeline_{event}'] = pipeline_value
                
                # Calculate difference
                if gt_value is not None and pipeline_value is not None:
                    diff = pipeline_value - gt_value
                    comparison_row[f'diff_{event}'] = diff
                    abs_diff = abs(diff)
                    comparison_row[f'abs_diff_{event}'] = abs_diff
                    
                    print(f"  {event}: GT={gt_value}, Pipeline={pipeline_value}, Diff={diff} ({abs_diff} frames)")
                else:
                    comparison_row[f'diff_{event}'] = None
                    comparison_row[f'abs_diff_{event}'] = None
                    
                    if gt_value is None and pipeline_value is None:
                        print(f"  {event}: Both missing")
                    elif gt_value is None:
                        print(f"  {event}: GT missing, Pipeline={pipeline_value}")
                    else:
                        print(f"  {event}: GT={gt_value}, Pipeline missing")
            
            comparison_rows.append(comparison_row)
        
        self.comparison_results = pd.DataFrame(comparison_rows)
        
        print(f"\nComparison complete: {len(self.comparison_results)} trials")
        
        return self.comparison_results
    
    def generate_summary_statistics(self) -> Dict[str, Any]:
        """Generate summary statistics for the timing comparison."""
        if self.comparison_results is None:
            raise ValueError("No comparison results available. Call compare_timing() first.")
        
        print("\n=== TIMING VALIDATION SUMMARY ===")
        
        summary = {
            'total_trials': len(self.comparison_results),
            'events': {}
        }
        
        timing_events = ['latency_to_hunt', 'latency_to_attack', 'hunting_duration']
        
        for event in timing_events:
            gt_col = f'gt_{event}'
            pipeline_col = f'pipeline_{event}'
            diff_col = f'diff_{event}'
            abs_diff_col = f'abs_diff_{event}'
            
            # Count available comparisons
            valid_comparisons = self.comparison_results[
                self.comparison_results[gt_col].notna() & 
                self.comparison_results[pipeline_col].notna()
            ]
            
            gt_available = self.comparison_results[gt_col].notna().sum()
            pipeline_available = self.comparison_results[pipeline_col].notna().sum()
            both_available = len(valid_comparisons)
            
            event_summary = {
                'gt_available': gt_available,
                'pipeline_available': pipeline_available, 
                'both_available': both_available,
                'comparison_rate': both_available / gt_available if gt_available > 0 else 0
            }
            
            if both_available > 0:
                differences = valid_comparisons[diff_col]
                abs_differences = valid_comparisons[abs_diff_col]
                
                event_summary.update({
                    'mean_diff': differences.mean(),
                    'std_diff': differences.std(),
                    'median_diff': differences.median(),
                    'mean_abs_diff': abs_differences.mean(),
                    'median_abs_diff': abs_differences.median(),
                    'max_abs_diff': abs_differences.max(),
                    'q25_abs_diff': abs_differences.quantile(0.25),
                    'q75_abs_diff': abs_differences.quantile(0.75)
                })
            
            summary['events'][event] = event_summary
            
            # Print summary
            print(f"\n{event.upper()}:")
            print(f"  Ground truth available: {gt_available}/{summary['total_trials']}")
            print(f"  Pipeline predictions: {pipeline_available}/{summary['total_trials']}")
            print(f"  Comparable pairs: {both_available}/{gt_available} ({event_summary['comparison_rate']*100:.1f}%)")
            
            if both_available > 0:
                print(f"  Mean difference: {event_summary['mean_diff']:.1f} Â± {event_summary['std_diff']:.1f} frames")
                print(f"  Median difference: {event_summary['median_diff']:.1f} frames")
                print(f"  Mean absolute difference: {event_summary['mean_abs_diff']:.1f} frames")
                print(f"  Median absolute difference: {event_summary['median_abs_diff']:.1f} frames")
        
        return summary
    
    def create_validation_plots(self, output_dir: str) -> None:
        """Create validation plots and save to output directory."""
        if self.comparison_results is None:
            raise ValueError("No comparison results available. Call compare_timing() first.")
        
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        timing_events = ['latency_to_hunt', 'latency_to_attack', 'hunting_duration']
        
        # 1. Scatter plots: Ground Truth vs Pipeline Predictions
        fig, axes = plt.subplots(1, 3, figsize=(18, 5))
        
        for i, event in enumerate(timing_events):
            gt_col = f'gt_{event}'
            pipeline_col = f'pipeline_{event}'
            
            # Get valid data points
            valid_data = self.comparison_results[
                self.comparison_results[gt_col].notna() & 
                self.comparison_results[pipeline_col].notna()
            ]
            
            if len(valid_data) > 0:
                axes[i].scatter(valid_data[gt_col], valid_data[pipeline_col], alpha=0.6)
                
                # Add perfect correlation line
                min_val = min(valid_data[gt_col].min(), valid_data[pipeline_col].min())
                max_val = max(valid_data[gt_col].max(), valid_data[pipeline_col].max())
                axes[i].plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8, label='Perfect match')
                
                axes[i].set_xlabel('Ground Truth (frames)')
                axes[i].set_ylabel('Pipeline Prediction (frames)')
                axes[i].set_title(f'{event.replace("_", " ").title()}\nn={len(valid_data)}')
                axes[i].legend()
                
                # Calculate and show correlation
                correlation = valid_data[gt_col].corr(valid_data[pipeline_col])
                axes[i].text(0.05, 0.95, f'r = {correlation:.3f}', 
                           transform=axes[i].transAxes, verticalalignment='top',
                           bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
            else:
                axes[i].text(0.5, 0.5, 'No valid\ncomparisons', 
                           transform=axes[i].transAxes, ha='center', va='center')
                axes[i].set_title(f'{event.replace("_", " ").title()}\nn=0')
        
        plt.tight_layout()
        plt.savefig(output_path / 'timing_correlation_plots.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. Difference distribution plots
        fig, axes = plt.subplots(1, 3, figsize=(18, 5))
        
        for i, event in enumerate(timing_events):
            diff_col = f'diff_{event}'
            
            # Get valid differences
            valid_diffs = self.comparison_results[self.comparison_results[diff_col].notna()][diff_col]
            
            if len(valid_diffs) > 0:
                axes[i].hist(valid_diffs, bins=20, alpha=0.7, edgecolor='black')
                axes[i].axvline(0, color='red', linestyle='--', alpha=0.8, label='Perfect match')
                axes[i].axvline(valid_diffs.mean(), color='green', linestyle='--', alpha=0.8, 
                              label=f'Mean: {valid_diffs.mean():.1f}')
                
                axes[i].set_xlabel('Difference (Pipeline - Ground Truth) frames')
                axes[i].set_ylabel('Frequency')
                axes[i].set_title(f'{event.replace("_", " ").title()} Differences\nn={len(valid_diffs)}')
                axes[i].legend()
            else:
                axes[i].text(0.5, 0.5, 'No valid\ncomparisons', 
                           transform=axes[i].transAxes, ha='center', va='center')
                axes[i].set_title(f'{event.replace("_", " ").title()}\nn=0')
        
        plt.tight_layout()
        plt.savefig(output_path / 'timing_difference_distributions.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # 3. Per-animal comparison
        animals = self.comparison_results['animal_id'].unique()
        
        if len(animals) > 1:
            fig, ax = plt.subplots(figsize=(12, 8))
            
            # Create summary by animal
            animal_summaries = []
            for animal in animals:
                animal_data = self.comparison_results[self.comparison_results['animal_id'] == animal]
                
                for event in timing_events:
                    abs_diff_col = f'abs_diff_{event}'
                    valid_diffs = animal_data[animal_data[abs_diff_col].notna()][abs_diff_col]
                    
                    if len(valid_diffs) > 0:
                        animal_summaries.append({
                            'animal': animal,
                            'event': event,
                            'mean_abs_diff': valid_diffs.mean(),
                            'n_trials': len(valid_diffs)
                        })
            
            if animal_summaries:
                summary_df = pd.DataFrame(animal_summaries)
                
                # Pivot for plotting
                pivot_df = summary_df.pivot(index='animal', columns='event', values='mean_abs_diff')
                
                # Create grouped bar plot
                pivot_df.plot(kind='bar', ax=ax, rot=45)
                ax.set_ylabel('Mean Absolute Difference (frames)')
                ax.set_title('Timing Accuracy by Animal')
                ax.legend(title='Event', bbox_to_anchor=(1.05, 1), loc='upper left')
                
                plt.tight_layout()
                plt.savefig(output_path / 'timing_accuracy_by_animal.png', dpi=300, bbox_inches='tight')
                plt.close()
        
        print(f"Validation plots saved to: {output_path}")
    
    def save_detailed_results(self, output_path: str) -> None:
        """Save detailed comparison results to CSV."""
        if self.comparison_results is None:
            raise ValueError("No comparison results available. Call compare_timing() first.")
        
        self.comparison_results.to_csv(output_path, index=False)
        print(f"Detailed results saved to: {output_path}")
    
    def run_complete_validation(self, pipeline_results_dir: str, output_dir: str) -> Dict[str, Any]:
        """Run complete validation pipeline."""
        print("ğŸ¯ STARTING COMPLETE TIMING VALIDATION")
        
        # Load data
        self.load_ground_truth()
        self.load_pipeline_results(pipeline_results_dir)
        
        # Compare timing
        self.compare_timing()
        
        # Generate summary
        summary = self.generate_summary_statistics()
        
        # Create output directory
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # Save results
        self.save_detailed_results(output_path / 'timing_comparison_detailed.csv')
        self.create_validation_plots(output_dir)
        
        # Save summary to JSON
        import json
        with open(output_path / 'timing_validation_summary.json', 'w') as f:
            json.dump(summary, f, indent=2, default=str)
        
        print(f"\nâœ… Complete validation finished!")
        print(f"Results saved to: {output_path}")
        
        return summary

def main():
    """Command line interface for timing validation."""
    import argparse
    
    parser = argparse.ArgumentParser(description="Validate pipeline timing predictions")
    parser.add_argument("--ground-truth", required=True, help="Ground truth CSV file")
    parser.add_argument("--pipeline-results", required=True, help="Pipeline results directory") 
    parser.add_argument("--output-dir", required=True, help="Output directory for results")
    
    args = parser.parse_args()
    
    try:
        validator = TimingValidator(args.ground_truth)
        summary = validator.run_complete_validation(args.pipeline_results, args.output_dir)
        
        print(f"\nğŸ‰ Validation complete! Check {args.output_dir} for detailed results.")
        
    except Exception as e:
        print(f"âŒ Validation failed: {e}")
        import traceback
        traceback.print_exc()
        exit(1)

if __name__ == "__main__":
    main()